# 对话摘要任务配置 - SAMSum数据集
# 用于训练Transformer进行对话摘要

# 模型配置
d_model: 256
num_heads: 8
num_encoder_layers: 4
num_decoder_layers: 4
d_ff: 1024
dropout: 0.1
max_len: 512

# 训练配置
batch_size: 16
learning_rate: 3e-4
num_epochs: 30
warmup_steps: 500
max_grad_norm: 1.0
label_smoothing: 0.1

# 梯度累积配置
# 有效batch size = batch_size * gradient_accumulation_steps
# 例如：batch_size=16, gradient_accumulation_steps=2 => 有效batch_size=32
gradient_accumulation_steps: 2

# 早停配置
early_stopping_patience: 5
early_stopping_min_delta: 0.0

# 数据配置
dataset_name: "knkarthick/samsum"
vocab_size: 8000  # BPE分词器词表大小（从30k减少到8k）
tokenizer_path: "tokenizer.json"  # 自训练BPE分词器保存路径
max_src_len: 512  # 对话最大长度
max_tgt_len: 128  # 摘要最大长度

# 其他配置
seed: 42
device: auto  # 设备选择: auto (自动检测), mps (Apple Silicon), cuda (NVIDIA GPU), cpu
save_dir: results/summarization
log_interval: 50
